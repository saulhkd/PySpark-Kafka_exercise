# PySpark_exercise
En este ejercicio trabajarÃ¡s con **datos de sensores generados en streaming** y procesados con **PySpark**.  
El objetivo es que practiques desde la lectura de datos en tiempo real hasta anÃ¡lisis y agregaciones avanzadas, **dentro de un entorno Dockerizado con Jupyter y PySpark**.

### ğŸ”¹ Flujo general de los datos:

1. **Entorno Docker**  
   - Se levanta un contenedor con **Jupyter Notebook + PySpark**.  
   - TambiÃ©n se levantan contenedores para **Kafka** y **Zookeeper**, simulando un entorno de producciÃ³n real.  
   - Se accede al notebook mediante la URL que Spark imprime al iniciar Jupyter (sin necesidad de contraseÃ±a).

2. **Lectura desde Kafka**  
   Los datos de los sensores llegan continuamente a un topic de Kafka. Cada mensaje contiene informaciÃ³n como: `sensor_id`, `value`, `temperature`, `humidity`, `status`, `timestamp` y `uuid`.

3. **Parseo y transformaciÃ³n**  
   Los mensajes JSON se transforman en columnas individuales, y se crea una columna `event_time` que convierte el timestamp a formato de fecha/hora. Esto permite trabajar con ventanas temporales y agregaciones.

4. **Agregaciones por ventana**  
   Los datos se agrupan en **ventanas de tiempo de 30 segundos** por cada sensor, calculando mÃ©tricas como:
   - Promedio del valor (`avg_value`)  
   - Promedio de la temperatura (`avg_temp`)  
   - Promedio de la humedad (`avg_humidity`)  
   - NÃºmero de eventos (`num_events`)  
   AdemÃ¡s, se utiliza un **watermark de 1 minuto** para manejar retrasos y datos tardÃ­os.

5. **Escritura en archivos Parquet**  
   Los resultados agregados se escriben continuamente en archivos Parquet en la carpeta `resultados/`.  
   Se usa un **checkpoint** para asegurar que Spark recuerde quÃ© datos ya se procesaron, evitando duplicados si el streaming se reinicia.

### ğŸ”¹ QuÃ© vas a practicar en este ejercicio:

- Levantar un entorno **Dockerizado** con Jupyter, PySpark y Kafka  
- Lectura de datos en **streaming** desde Kafka  
- TransformaciÃ³n y parseo de mensajes JSON  
- CreaciÃ³n y uso de **ventanas de tiempo**  
- Agregaciones y estadÃ­sticas sobre los datos de sensores  
- Escritura continua de resultados en **Parquet**  
- ExploraciÃ³n y anÃ¡lisis de los resultados agregados  

Al final del ejercicio, tendrÃ¡s un conjunto de datos consolidados que podrÃ¡s **consultar y analizar** para responder preguntas sobre comportamiento de los sensores, detecciÃ³n de anomalÃ­as, ranking, y mÃ¡s.

# PASOS A SEGUIR IMPORTANTE

1. En una terminal levantar el docker con el siguiente comando
   - docker-compose up --build
2. Haceros un split de la terminal y ejecutar el productor de mensajes de kafka desde dentro
del contenedor
   - docker exec -it pyspark_lab bash
   - python kafka-producer/producer.py
3. Abrir una nueva terminal y ejecutar el comando para obtener la URL que podeis usar
   - docker-compose logs -f jupyter

Teneis todos estos comando en el archivo comandos.txt para que lo veais mas fÃ¡cil, cualquier duda para levantar el entorno no dudeis en comunicarmela sin ningÃºn problema. 

# ğŸ“˜ Ejercicio â€“ PySpark & Streaming
Procesamiento y anÃ¡lisis de datos agregados por ventana.

Este ejercicio estÃ¡ diseÃ±ado para evaluar tu dominio de PySpark aplicado a procesamiento de datos reales provenientes de un flujo de streaming. TrabajarÃ¡s con el DataFrame final generado tras las agregaciones, el cual contiene:

- `window` â€“ Ventana temporal (start, end)
- `sensor_id` â€“ Identificador del sensor
- `avg_value` â€“ Valor promedio de la mÃ©trica principal
- `avg_temp` â€“ Temperatura promedio
- `avg_humidity` â€“ Humedad promedio
- `num_events` â€“ NÃºmero de eventos agregados

---

## ğŸ§° 0. PreparaciÃ³n
AsegÃºrate de:

- Haber cargado correctamente el DataFrame producido por el streaming.
- Eliminar cualquier columna creada manualmente o que no forme parte del esquema original.

---

## ğŸ” 1. ExploraciÃ³n inicial
Realiza una exploraciÃ³n bÃ¡sica del DataFrame:

- Muestra el esquema completo.
- Indica cuÃ¡ntas filas contiene.
- Â¿CuÃ¡ntos sensores distintos (`sensor_id`) aparecen?

---

## ğŸ”§ 2. Transformaciones numÃ©ricas
Crea una columna basada en la diferencia o relaciÃ³n entre dos mÃ©tricas numÃ©ricas del DataFrame.

Responde:

- Â¿CuÃ¡l es el valor mÃ­nimo, mÃ¡ximo y medio de la nueva columna?

---

## ğŸ§¹ 3. Filtrado avanzado
Aplica un filtro usando varias condiciones a la vez relacionadas con:

- humedad,
- nÃºmero de eventos,
- sensor.

Responde:

- Â¿CuÃ¡ntos registros cumplen todas las condiciones aplicadas?

---

## ğŸ“Š 4. Agregaciones por sensor
Agrupa el DataFrame por `sensor_id` y obtÃ©n estadÃ­sticas descriptivas.

Responde:

- Â¿QuÃ© sensor tiene la mayor media en la variable analizada?
- Â¿QuÃ© sensor presenta la temperatura mÃ¡xima?
- Â¿CuÃ¡ntos grupos o ventanas existen por sensor?

---

## ğŸ… 5. Ranking con funciones de ventana
Usa funciones de ventana para identificar, para cada sensor, el registro con el mayor valor de `avg_value`.

Responde:

- Â¿QuÃ© sensor obtiene el valor mÃ¡ximo global entre todos?

---

## ğŸ”— 6. Join con tabla auxiliar
Crea un DataFrame auxiliar con informaciÃ³n adicional sobre sensores (por ejemplo, categorÃ­a, ubicaciÃ³n o tipo) y realiza un join.

Responde:

- Muestra los cinco primeros registros del DataFrame ya unido.

---

## ğŸ•’ 7. AgrupaciÃ³n por ventana temporal
Extrae la marca de inicio de la ventana y agrÃºpala por unidades de tiempo truncadas (p. ej., minuto u hora).

Responde:

- Â¿CuÃ¡ntas ventanas hay por unidad temporal?
- Â¿CuÃ¡l es la humedad media por cada unidad?

---

## ğŸ§© 8. ReparticiÃ³n y particiones
Reparte el DataFrame por `sensor_id` y aÃ±ade una columna indicando el identificador de particiÃ³n.

Responde:

- Â¿CuÃ¡l es la distribuciÃ³n de registros entre las particiciones?

---

## ğŸš¨ 9. DetecciÃ³n de anomalÃ­as
Define un criterio personalizado de "anomalÃ­a" basado en los valores medios del DataFrame.

Responde:

- Â¿CuÃ¡ntos registros son anÃ³malos?
- Â¿QuÃ© sensor tiene mÃ¡s anomalÃ­as?

---

## ğŸ”¢ 10. Cruce de mÃ©tricas
Crea una puntuaciÃ³n combinando varias columnas del DataFrame (por ejemplo, ponderando `avg_value`, temperatura y humedad).

Responde:

- Â¿QuÃ© sensor obtiene la puntuaciÃ³n mÃ¡s alta?
- Â¿CuÃ¡l es el valor de dicha puntuaciÃ³n?

---

## ğŸ”¢ 11. Graficar DF
Crea dos grÃ¡ficos, convirtiendo el DF de spark a pandas y utiliza la libreria matplotlib 
para generar dos grÃ¡ficos sobre alguna de las variables.

![Imagen](image.png)